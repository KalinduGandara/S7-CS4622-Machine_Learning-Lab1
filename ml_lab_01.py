# -*- coding: utf-8 -*-
"""ML_Lab_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZsAkiZSx7vpfpDPQ42I5wnFEE6fY5ZbC
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

"""#1. Import Data and Analysis"""

train_df = pd.read_csv("/content/drive/MyDrive/ML/Lab 01/train.csv")
valid_df = pd.read_csv("/content/drive/MyDrive/ML/Lab 01/valid.csv")
test_df = pd.read_csv("/content/drive/MyDrive/ML/Lab 01/test.csv")

LABELS = ["label_1","label_2","label_3","label_4"]
train_X_dict = {}
train_y_dict = {}
valid_X_dict = {}
valid_y_dict = {}
FEATURES = train_df.columns[:-4]

# nearly 70% is value 6
train_df['label_4'].value_counts()/train_df.shape[0]

test_df = test_df.drop(columns=LABELS)
test_df

train_nan = train_df.isna().sum()
print("Train\n",train_nan[train_nan>0])
valid_nan = valid_df.isna().sum()
print("Valid\n",valid_nan[valid_nan>0])
test_nan = test_df.isna().sum()
print("Test\n",test_nan[test_nan>0])

all_feature_scalers = {}
for label in LABELS:
    scaler = StandardScaler()
    train_temp = train_df.dropna(subset=[label])
    train_X_dict[label] = pd.DataFrame(scaler.fit_transform(train_temp.drop(LABELS,axis=1)),columns=FEATURES)
    train_y_dict[label] = train_temp[label]

    valid_temp = valid_df.dropna(subset=[label])
    valid_X_dict[label] = pd.DataFrame(scaler.transform(valid_temp.drop(LABELS,axis=1)),columns=FEATURES)
    valid_y_dict[label] = valid_temp[label]

    all_feature_scalers[label] = scaler

"""# 2. train form all features"""

def print_accurcy_and_get_model(X,y,X_valid,y_valid):
  model=SVC(kernel='linear',class_weight='balanced')
  model.fit(X,y)
  y_pred = model.predict(X_valid)
  print(accuracy_score(y_pred,y_valid))
  return model

all_feature_models = {}

for label in LABELS:
  all_feature_models[label] = print_accurcy_and_get_model(train_X_dict[label],train_y_dict[label],valid_X_dict[label],valid_y_dict[label])

"""# 3. Feature Removal


"""

correlation_matrix =train_X_dict[LABELS[0]].corr()

# Set the correlation threshold
threshold = 0.8

# Find columns with correlations higher than the threshold
high_correlation_columns = []
for col in correlation_matrix.columns:
    correlated_cols = correlation_matrix[correlation_matrix[col] > threshold].index.tolist()
    correlated_cols.remove(col)  # Remove self-correlation
    if correlated_cols:
        high_correlation_columns.append(col)

print("Columns with correlations higher than 0.8:")
print(high_correlation_columns)

from sklearn.feature_selection import VarianceThreshold
v_threshold = VarianceThreshold(threshold=0)
v_threshold.fit(train_X_dict[LABELS[0]]) # fit finds the features with zero variance
v_threshold.get_support( )

"""# 4. Feature Selection"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

selectKBest_models = {}
selectKBest_X = {}
selectKBest_selecters = {}
for label in LABELS:
  selecter = SelectKBest(f_classif, k=100)
  X_train_new = selecter.fit_transform(train_X_dict[label], train_y_dict[label])
  X_valid_new = selecter.transform(valid_X_dict[label])
  selectKBest_models[label] = print_accurcy_and_get_model(X_train_new,train_y_dict[label],X_valid_new,valid_y_dict[label])
  print("Diamentions ",X_train_new.shape)
  selectKBest_X[label] = X_train_new
  selectKBest_selecters[label] = selecter
# a = SelectKBest(f_classif, k=100).fit_transform(train_X_dict[label], train_y_dict[label])

from sklearn.metrics.cluster import pair_confusion_matrix
from sklearn.decomposition import PCA

pca_models = {}
pca_X = {}
pcas = {}
for label in LABELS:
  pca = PCA(n_components=0.99,svd_solver='full')
  X_train_new = pca.fit_transform(train_X_dict[label], train_y_dict[label])
  X_valid_new = pca.transform(valid_X_dict[label])
  pca_models[label] = print_accurcy_and_get_model(X_train_new,train_y_dict[label],X_valid_new,valid_y_dict[label])
  print("Diamentions ",X_train_new.shape)
  pca_X[label] = X_train_new
  pcas[label] = pca

"""# 5. Export CSV

best prediction is PCA prediction
"""

all_pred = {}
best_pred = {}
new_features = {}
for label in LABELS:
  all_pred[label] = all_feature_models[label].predict(all_feature_scalers[label].transform(test_df))
  new = pcas[label].transform(test_df)
  new_features[label] = new
  best_pred[label] = pca_models[label].predict(new)

initial_labels = ["Predicted labels before feature engineering", "Predicted labels after feature engineering","No of new features"]
new_feature_labels = [f"new_feature_{i}" for i in range(1,107)]
new_columns = initial_labels + new_feature_labels

for label in LABELS:
  data = {column_name: array for column_name, array in zip(new_columns, [all_pred[label], best_pred[label],new_features[label].shape[1]] + [new_features[label][:, i] for i in range(106)])}
  # creting empty exrta columns
  for i in range(107,257):
    data[f"new_feature_{i}"] = np.nan
  df = pd.DataFrame(data)
  df.to_csv(f"/content/drive/MyDrive/ML/Lab 01/submit/190453A_{label}.csv",index=False)